{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13827843,"sourceType":"datasetVersion","datasetId":8806448}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-24T15:46:51.930182Z","iopub.execute_input":"2025-11-24T15:46:51.930760Z","iopub.status.idle":"2025-11-24T15:46:53.538717Z","shell.execute_reply.started":"2025-11-24T15:46:51.930719Z","shell.execute_reply":"2025-11-24T15:46:53.537806Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/classes/istockphoto-1130206284-640_adpp_is.mp4\n/kaggle/input/classes/istockphoto-1164175351-640_adpp_is.mp4\n/kaggle/input/classes/istockphoto-1056900582-640_adpp_is.mp4\n/kaggle/input/classes/3998516-uhd_4096_2160_25fps.mp4\n/kaggle/input/classes/istockphoto-1413752945-640_adpp_is.mp4\n/kaggle/input/classes/istockphoto-2165346142-640_adpp_is.mp4\n/kaggle/input/classes/2519660-uhd_3840_2160_24fps.mp4\n/kaggle/input/classes/istockphoto-1264336655-640_adpp_is.mp4\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"video_path=\"/kaggle/input/classes/3998516-uhd_4096_2160_25fps.mp4\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T16:14:17.958725Z","iopub.execute_input":"2025-11-24T16:14:17.959022Z","iopub.status.idle":"2025-11-24T16:14:17.963230Z","shell.execute_reply.started":"2025-11-24T16:14:17.959003Z","shell.execute_reply":"2025-11-24T16:14:17.962356Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Install required packages\n!pip install -q git+https://github.com/huggingface/transformers\n!pip install -q accelerate qwen-vl-utils av\n\nimport torch\nfrom transformers import Qwen2VLForConditionalGeneration, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\n\n# Load model - using 2B version for faster inference\nmodel_id = \"Qwen/Qwen2-VL-2B-Instruct\"\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n    model_id,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\nprocessor = AutoProcessor.from_pretrained(model_id)\ndef generate_video_summary(video_path, classification_info=\"\"):\n    \"\"\"\n    Generate summary for a video with optional classification context.\n    The model is explicitly forced to use the provided classification info exactly as given.\n    \"\"\"\n\n    # Strong ground-truth enforcing prompt\n    if classification_info:\n        text_prompt = (\n            \"Treat the following classification information as ABSOLUTE TRUTH. \"\n            \"You are NOT allowed to reinterpret it, modify it, or ignore it.\\n\\n\"\n            f\"CLASSIFICATION (GROUND TRUTH):\\n{classification_info}\\n\\n\"\n            \"You MUST include the following in your answer:\\n\"\n            \"- The classification EXACTLY as given.\\n\"\n            \"- The fine-grained class EXACTLY as given.\\n\"\n            \"- A description of what happens in the video.\\n\"\n            \"Do not omit or alter any required element.\"\n        )\n    else:\n        text_prompt = (\n            \"Summarize the video briefly (2–3 lines). \"\n            \"Include details.\"\n        )\n\n\n    # Prepare messages\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"video\", \"video\": video_path},\n                {\"type\": \"text\", \"text\": text_prompt},\n            ],\n        }\n    ]\n    \n    # Process inputs\n    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    image_inputs, video_inputs = process_vision_info(messages)\n    \n    inputs = processor(\n        text=[text],\n        images=image_inputs,\n        videos=video_inputs,\n        padding=True,\n        return_tensors=\"pt\"\n    ).to(model.device)\n    \n    # Generate response\n    with torch.no_grad():\n        output_ids = model.generate(\n            **inputs,\n            max_new_tokens=256,\n            do_sample=True,\n            temperature=0.7,\n            top_p=0.9\n        )\n    \n    # Decode output\n    generated_ids = [\n        output_ids[len(input_ids):]\n        for input_ids, output_ids in zip(inputs.input_ids, output_ids)\n    ]\n    \n    summary = processor.batch_decode(\n        generated_ids,\n        skip_special_tokens=True,\n        clean_up_tokenization_spaces=True\n    )[0]\n    \n    return summary\n\n# Example 1: Without classification info\nprint(\"=== Summary without classification context ===\")\nsummary1 = generate_video_summary(video_path)\nprint(summary1)\nprint()\n\n# Example 2: With normal classification\nprint(\"=== Summary with normal classification ===\")\nclassification_result = \"Model Classification: Normal Activity\\nFine-grained Class: Haircut\"\nsummary2 = generate_video_summary(video_path, classification_result)\nprint(summary2)\nprint()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T16:14:43.576493Z","iopub.execute_input":"2025-11-24T16:14:43.576845Z","iopub.status.idle":"2025-11-24T16:15:03.037036Z","shell.execute_reply.started":"2025-11-24T16:14:43.576825Z","shell.execute_reply":"2025-11-24T16:15:03.036221Z"}},"outputs":[{"name":"stdout","text":"=== Summary without classification context ===\nThe video shows a man getting his beard trimmed with an electric razor by a barber in a barber shop. The barber is wearing a white shirt with a decorative bracelet on his wrist. The background features various barber shop equipment and lighting.\n\n=== Summary with normal classification ===\nThe classification EXACTLY as given is \"Normal Activity\".\nThe fine-grained class EXACTLY as given is \"Haircut\".\nIn the video, a barber is using an electric razor to trim a man's beard. The barber is focused on the task, and the man is seated in a chair. The background shows various barber shop equipment and tools.\n\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"def generate_video_summary(video_path, classification_info=\"\"):\n    \"\"\"\n    Generate summary for a video with optional classification context.\n    The model is explicitly forced to use the provided classification info exactly as given.\n    \"\"\"\n\n    \n    text_prompt = (\n        \"You MUST treat the following classification results as GROUND TRUTH. \"\n        \"You MUST extract them EXACTLY and place them in the correct fields below.\\n\\n\"\n        f\"{classification_info}\\n\\n\"\n        \"Extract the following EXACT fields:\\n\"\n        \"- 'Model Classification' → goes into the 'Classification' field\\n\"\n        \"- 'Fine-grained Class' → goes into the 'Fine-grained Class' field\\n\\n\"\n        \"Now output in EXACTLY this format:\\n\\n\"\n        \"Classification: <insert Model Classification>\\n\"\n        \"Fine-grained Class: <insert Fine-grained Class>\\n\"\n        \"Video Description: <describe what happens in the video>\\n\"\n        \"Do NOT swap the fields. Do NOT rewrite the labels. Do NOT change wording.\"\n    )\n\n\n    # Prepare messages\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"video\", \"video\": video_path},\n                {\"type\": \"text\", \"text\": text_prompt},\n            ],\n        }\n    ]\n    \n    # Process inputs\n    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    image_inputs, video_inputs = process_vision_info(messages)\n    \n    inputs = processor(\n        text=[text],\n        images=image_inputs,\n        videos=video_inputs,\n        padding=True,\n        return_tensors=\"pt\"\n    ).to(model.device)\n    \n    # Generate response\n    with torch.no_grad():\n        output_ids = model.generate(\n            **inputs,\n            max_new_tokens=256,\n            do_sample=True,\n            temperature=0.7,\n            top_p=0.9\n        )\n    \n    # Decode output\n    generated_ids = [\n        output_ids[len(input_ids):]\n        for input_ids, output_ids in zip(inputs.input_ids, output_ids)\n    ]\n    \n    summary = processor.batch_decode(\n        generated_ids,\n        skip_special_tokens=True,\n        clean_up_tokenization_spaces=True\n    )[0]\n    \n    return summary\n\n\n# Example 2: With normal classification\nprint(\"=== Summary with normal classification ===\")\nclassification_result = \"Model Classification: Normal Activity\\nFine-grained Class: Haircut\"\nsummary2 = generate_video_summary(video_path, classification_result)\nprint(summary2)\nprint()\n\n# Example 3: With abnormal classification\n# print(\"=== Summary with abnormal classification ===\")\n# classification_result = \"Model Classification: Abnormal Activity\\nAbnormal Type: Burglary\"\n# summary3 = generate_video_summary(video_path, classification_result)\n# print(summary3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T16:15:03.038165Z","iopub.execute_input":"2025-11-24T16:15:03.038440Z","iopub.status.idle":"2025-11-24T16:15:13.930805Z","shell.execute_reply.started":"2025-11-24T16:15:03.038414Z","shell.execute_reply":"2025-11-24T16:15:13.930029Z"}},"outputs":[{"name":"stdout","text":"=== Summary with normal classification ===\nClassification: Normal Activity\nFine-grained Class: Haircut\nVideo Description: A man is getting his hair cut by a barber in a barber shop. The barber is using a hair clipper to trim the man's hair and beard.\n\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"\ndef generate_video_summary(video_path, classification_info=\"\"):\n    \"\"\"\n    Generate summary for a video with optional classification context.\n    The model is explicitly forced to use the provided classification info exactly as given.\n    \"\"\"\n\n   \n    text_prompt = (\n        \"Follow the classification information EXACTLY as provided. \"\n        \"You may comment on whether the video content agrees with it, but NEVER alter the classification text.\\n\\n\"\n        f\"Classification Provided:\\n{classification_info}\\n\\n\"\n        \"Your output must include:\\n\"\n        \"1. The provided classification as given.\\n\"\n        \"2. A clear description of the video's content.\\n\"\n    )\n\n\n\n    # Prepare messages\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"video\", \"video\": video_path},\n                {\"type\": \"text\", \"text\": text_prompt},\n            ],\n        }\n    ]\n    \n    # Process inputs\n    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    image_inputs, video_inputs = process_vision_info(messages)\n    \n    inputs = processor(\n        text=[text],\n        images=image_inputs,\n        videos=video_inputs,\n        padding=True,\n        return_tensors=\"pt\"\n    ).to(model.device)\n    \n    # Generate response\n    with torch.no_grad():\n        output_ids = model.generate(\n            **inputs,\n            max_new_tokens=256,\n            do_sample=True,\n            temperature=0.7,\n            top_p=0.9\n        )\n    \n    # Decode output\n    generated_ids = [\n        output_ids[len(input_ids):]\n        for input_ids, output_ids in zip(inputs.input_ids, output_ids)\n    ]\n    \n    summary = processor.batch_decode(\n        generated_ids,\n        skip_special_tokens=True,\n        clean_up_tokenization_spaces=True\n    )[0]\n    \n    return summary\n\n\n\n# Example 2: With normal classification\nprint(\"=== Summary with normal classification ===\")\nclassification_result = \"Model Classification: Normal Activity\\nFine-grained Class: Haircut\"\nsummary2 = generate_video_summary(video_path, classification_result)\nprint(summary2)\nprint()\n\n# Example 3: With abnormal classification\n# print(\"=== Summary with abnormal classification ===\")\n# classification_result = \"Model Classification: Abnormal Activity\\nAbnormal Type: Burglary\"\n# summary3 = generate_video_summary(video_path, classification_result)\n# print(summary3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T16:16:18.884209Z","iopub.execute_input":"2025-11-24T16:16:18.884915Z","iopub.status.idle":"2025-11-24T16:16:28.189403Z","shell.execute_reply.started":"2025-11-24T16:16:18.884893Z","shell.execute_reply":"2025-11-24T16:16:28.188737Z"}},"outputs":[{"name":"stdout","text":"=== Summary with normal classification ===\nThe video shows a close-up of a barber using an electric razor to trim a man's beard. The background is blurred, focusing attention on the barber's hands and the man's hair and beard. The setting appears to be a barber shop or a similar hair salon.\n\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"\n# def generate_video_summary(video_path, classification_info=\"\"):\n#     \"\"\"\n#     Generate summary for a video with optional classification context.\n#     The model is explicitly forced to use the provided classification info exactly as given.\n#     \"\"\"\n\n    \n#     text_prompt = (\n#         \"Incorporate the following classification into your summary. \"\n#         \"Do not change the labels, but you may expand on them.\\n\\n\"\n#         f\"Classification:\\n{classification_info}\\n\\n\"\n#         \"Explain what is happening in the video and indicate whether the visual content \"\n#         \"supports or contradicts the classification.\"\n#     )\n\n\n\n\n#     # Prepare messages\n#     messages = [\n#         {\n#             \"role\": \"user\",\n#             \"content\": [\n#                 {\"type\": \"video\", \"video\": video_path},\n#                 {\"type\": \"text\", \"text\": text_prompt},\n#             ],\n#         }\n#     ]\n    \n#     # Process inputs\n#     text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n#     image_inputs, video_inputs = process_vision_info(messages)\n    \n#     inputs = processor(\n#         text=[text],\n#         images=image_inputs,\n#         videos=video_inputs,\n#         padding=True,\n#         return_tensors=\"pt\"\n#     ).to(model.device)\n    \n#     # Generate response\n#     with torch.no_grad():\n#         output_ids = model.generate(\n#             **inputs,\n#             max_new_tokens=256,\n#             do_sample=True,\n#             temperature=0.7,\n#             top_p=0.9\n#         )\n    \n#     # Decode output\n#     generated_ids = [\n#         output_ids[len(input_ids):]\n#         for input_ids, output_ids in zip(inputs.input_ids, output_ids)\n#     ]\n    \n#     summary = processor.batch_decode(\n#         generated_ids,\n#         skip_special_tokens=True,\n#         clean_up_tokenization_spaces=True\n#     )[0]\n    \n#     return summary\n\n\n\n# # Example 2: With normal classification\n# print(\"=== Summary with normal classification ===\")\n# classification_result = \"Model Classification: Normal Activity\\nFine-grained Class: Haircut\"\n# summary2 = generate_video_summary(video_path, classification_result)\n# print(summary2)\n# print()\n\n# # Example 3: With abnormal classification\n# # print(\"=== Summary with abnormal classification ===\")\n# # classification_result = \"Model Classification: Abnormal Activity\\nAbnormal Type: Burglary\"\n# # summary3 = generate_video_summary(video_path, classification_result)\n# # print(summary3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T15:59:29.031689Z","iopub.execute_input":"2025-11-24T15:59:29.032273Z","iopub.status.idle":"2025-11-24T15:59:29.037199Z","shell.execute_reply.started":"2025-11-24T15:59:29.032251Z","shell.execute_reply":"2025-11-24T15:59:29.036438Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"\ndef generate_video_summary(video_path, classification_info=\"\"):\n    \"\"\"\n    Generate summary for a video with optional classification context.\n    The model is explicitly forced to use the provided classification info exactly as given.\n    \"\"\"\n\n    text_prompt = (\n        \"You MUST copy the classification information EXACTLY as given, without any changes.\\n\\n\"\n        f\"{classification_info}\\n\\n\"\n        \"Your response MUST contain ONLY the following 3 sections:\\n\\n\"\n        \"1. CLASSIFICATION (copy EXACTLY from above)\\n\"\n        \"2. VIDEO SUMMARY (describe the video)\\n\"\n        \"Do NOT change section titles. Do NOT rewrite classification. Follow the format EXACTLY.\"\n    )\n    \n\n\n\n\n\n    # Prepare messages\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"video\", \"video\": video_path},\n                {\"type\": \"text\", \"text\": text_prompt},\n            ],\n        }\n    ]\n    \n    # Process inputs\n    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    image_inputs, video_inputs = process_vision_info(messages)\n    \n    inputs = processor(\n        text=[text],\n        images=image_inputs,\n        videos=video_inputs,\n        padding=True,\n        return_tensors=\"pt\"\n    ).to(model.device)\n    \n    # Generate response\n    with torch.no_grad():\n        output_ids = model.generate(\n            **inputs,\n            max_new_tokens=256,\n            do_sample=True,\n            temperature=0.7,\n            top_p=0.9\n        )\n    \n    # Decode output\n    generated_ids = [\n        output_ids[len(input_ids):]\n        for input_ids, output_ids in zip(inputs.input_ids, output_ids)\n    ]\n    \n    summary = processor.batch_decode(\n        generated_ids,\n        skip_special_tokens=True,\n        clean_up_tokenization_spaces=True\n    )[0]\n    \n    return summary\n\n\n\n# Example 2: With normal classification\nprint(\"=== Summary with normal classification ===\")\nclassification_result = \"Model Classification: Normal Activity\\nFine-grained Class: Haircut\"\nsummary2 = generate_video_summary(video_path, classification_result)\nprint(summary2)\nprint()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T16:15:22.903397Z","iopub.execute_input":"2025-11-24T16:15:22.903666Z","iopub.status.idle":"2025-11-24T16:15:32.362657Z","shell.execute_reply.started":"2025-11-24T16:15:22.903648Z","shell.execute_reply":"2025-11-24T16:15:32.361846Z"}},"outputs":[{"name":"stdout","text":"=== Summary with normal classification ===\n1. CLASSIFICATION: Normal Activity\n2. VIDEO SUMMARY: The video captures a close-up of a barber using an electric razor to trim a client's beard in a well-lit barbershop. The focus is on the barber's hands and the razor, with the background showing various barber tools and equipment.\n\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}